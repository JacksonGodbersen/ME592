{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.lr = lr\n",
    "        self.policy_net = DQN(input_size, output_size)\n",
    "        self.target_net = DQN(input_size, output_size)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.output_size)\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            return self.policy_net(state).argmax().item()\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward], dtype=torch.float32)\n",
    "        target = reward + self.gamma * torch.max(self.target_net(next_state).detach()) * (1 - done)\n",
    "        q_val = self.policy_net(state).gather(1, action.unsqueeze(1))\n",
    "        loss = self.loss_fn(q_val, target.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = DQNAgent(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "for episode in range(3000):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        agent.train(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        agent.update_target_net()\n",
    "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ufal.pybox2d in /home/exouser/.local/lib/python3.9/site-packages (2.3.10.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ufal.pybox2d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('LunarLander-v2',\n",
    "            gravity=-10.0,\n",
    "            enable_wind=True,\n",
    "            wind_power=15.0,\n",
    "            turbulence_power=1.5,\n",
    "            max_episode_steps=600,)\n",
    "env.reset(seed=42)\n",
    "\n",
    "# Play one complete episode with random actions\n",
    "while True:\n",
    "    action = env.action_space.sample() \n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\n\\nclass DQN(torch.nn.Module):\\n    def __init__(self, state_size=8, action_size=4, hidden_size=64, activation=torch.relu):\\n        super(DQN, self).__init__()\\n        self.layer1 = torch.nn.Linear(state_size, hidden_size)\\n        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\\n        self.layer3 = torch.nn.Linear(hidden_size, action_size)\\n        self.activation = activation\\n\\n    def forward(self, state):\\n        x = self.activation(self.layer1(state))\\n        x = self.activation(self.layer2(x))\\n        return self.layer3(x)\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, state_size=8, action_size=4, hidden_size=64, activation=torch.relu):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(state_size, hidden_size)\n",
    "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = torch.nn.Linear(hidden_size, action_size)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.activation(self.layer1(state))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, state_size=8, action_size=4, hidden_size=64, num_hidden_layers=2, activation=torch.relu):\n",
    "        super(DQN, self).__init__()\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        # Define the input layer\n",
    "        self.input_layer = torch.nn.Linear(state_size, hidden_size)\n",
    "        \n",
    "        # Define the hidden layers\n",
    "        self.hidden_layers = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden_layers)\n",
    "        ])\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.activation(self.input_layer(state))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.stack(states), actions, rewards, np.stack(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size=8, action_size=4, hidden_size=64, \n",
    "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64, optimizer='adam', activation=torch.relu, num_hidden_layers=2):\n",
    "        # Select device to train on (if CUDA available, use it, otherwise use CPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Discount factor for future rewards\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Batch size for sampling from the replay memory\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Number of possible actions\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Initialize the Q-Network and Target Network with the given state size, action size and hidden layer size\n",
    "        # Move the networks to the selected device\n",
    "        self.q_network = DQN(state_size, action_size, hidden_size, activation=activation).to(self.device)\n",
    "        self.target_network = DQN(state_size, action_size, hidden_size, activation=activation).to(self.device)\n",
    "        \n",
    "        # Set weights of target network to be the same as those of the q network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Set target network to evaluation mode\n",
    "        self.target_network.eval()\n",
    "\n",
    "        if optimizer == 'adam':\n",
    "            self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        elif optimizer == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Initialize the replay memory\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Store the experience in memory\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # If there are enough experiences in memory, perform a learning step\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.update_model()\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        # If a randomly chosen value is greater than eps\n",
    "        if random.random() > eps:  \n",
    "            # Convert state to a PyTorch tensor and set network to evaluation mode\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)  \n",
    "            self.q_network.eval()  \n",
    "\n",
    "            # With no gradient updates, get the action values from the DQN\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state)\n",
    "\n",
    "            # Revert to training mode and return action\n",
    "            self.q_network.train() \n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            # Return a random action for random value > eps\n",
    "            return random.choice(np.arange(self.action_size))  \n",
    "        \n",
    "    def update_model(self):\n",
    "        # Sample a batch of experiences from memory\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # Convert numpy arrays to PyTorch tensors\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        # Get Q-values for the actions that were actually taken\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Get maximum Q-value for the next states from target network\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
    "        \n",
    "        # Compute the expected Q-values\n",
    "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute the loss between the current and expected Q values\n",
    "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
    "        \n",
    "        # Zero all gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step the optimizer\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    scores_window.append(score)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def train(agent, env, n_episodes=5, eps_start=1.0, eps_end=0.01, eps_decay=0.995, target_update=10):\n",
    "    # Initialize the scores list and scores window\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "​\n",
    "    # Loop over episodes\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        \n",
    "        # Reset environment and score at the start of each episode\n",
    "        state, _ = env.reset()\n",
    "        score = 0 \n",
    "​\n",
    "        # Loop over steps\n",
    "        while True:\n",
    "            \n",
    "            # Select an action using current agent policy then apply in environment\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action) \n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Update the agent, state and score\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state \n",
    "            score += reward\n",
    "​\n",
    "            # End the episode if done\n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        # At the end of episode append and save scores\n",
    "        scores_window.append(score)\n",
    "        scores.append(score) \n",
    "​\n",
    "        # Decrease epsilon\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "​\n",
    "        # Print some info\n",
    "        #print(f\"\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\", end=\"\")\n",
    "​\n",
    "        # Update target network every target_update episodes\n",
    "        if i_episode % target_update == 0:\n",
    "            agent.update_target_network()\n",
    "            \n",
    "        # Print average score every 100 episodes\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "        # This environment is considered to be solved for a mean score of 200 or greater, so stop training.\n",
    "        if i_episode % 100 == 0 and np.mean(scores_window) >= 200:\n",
    "            break\n",
    "            \n",
    "​\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_scores(scores, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(scores, color='b', linestyle='-')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train and plot scores\n",
    "def train_and_plot(agent, env, title):\n",
    "    scores = train(agent, env)\n",
    "    plot_scores(scores, title)\n",
    "\n",
    "# Make an environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Create agents with different configurations\n",
    "agent_baseline = DQNAgent(state_size, action_size, optimizer='adam')\n",
    "agent_sgd = DQNAgent(state_size, action_size, optimizer='sgd')\n",
    "agent_tanh = DQNAgent(state_size, action_size, activation=torch.tanh)\n",
    "agent_leaky_ReLU = DQNAgent(state_size, action_size, activation=torch.nn.LeakyReLU()) # Default negative slope of .01\n",
    "agent_128 = DQNAgent(state_size, action_size, hidden_size=128)\n",
    "agent_256 = DQNAgent(state_size, action_size, hidden_size=256)\n",
    "agent_gamma_98 = DQNAgent(state_size, action_size, gamma=0.98)\n",
    "agent_gamma_95 = DQNAgent(state_size, action_size, gamma=0.95)\n",
    "agent_hidden_5 = DQNAgent(state_size, action_size, num_hidden_layers=5)\n",
    "agent_hidden_20 = DQNAgent(state_size, action_size, num_hidden_layers=20)\n",
    "\n",
    "# Train and plot scores for each agent\n",
    "train_and_plot(agent_baseline, env, \"Baseline\")\n",
    "train_and_plot(agent_sgd, env, \"SGD Optimizer\")\n",
    "train_and_plot(agent_tanh, env, \"Tanh Activation\")\n",
    "train_and_plot(agent_leaky_ReLU, env, \"Leaky ReLU Activation\")\n",
    "train_and_plot(agent_128, env, \"128 Hidden Units\")\n",
    "train_and_plot(agent_256, env, \"256 Hidden Units\")\n",
    "train_and_plot(agent_gamma_98, env, \"Discount Factor of 0.98\")\n",
    "train_and_plot(agent_gamma_95, env, \"Discount Factor of 0.95\")\n",
    "train_and_plot(agent_hidden_5, env, \"5 Hidden Layers\")\n",
    "train_and_plot(agent_hidden_20, env, \"20 Hidden Layers\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me592",
   "language": "python",
   "name": "me592"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
